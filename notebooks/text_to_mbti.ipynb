{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to MBTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib configuration\n",
    "mpl.rcParams['figure.figsize'] = 15, 15\n",
    "mpl.rcParams['figure.dpi'] = 300    # tells matplotlib to display inline plots at 300 DPI\n",
    "mpl.rc(\"savefig\", dpi = 300)        # tells matplotlib to save plots at 300 DPI\n",
    "plt.style.use('fivethirtyeight')\n",
    "# Makes Jupyter show the output of all lines, not just the last one\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/mbti.csv\")\n",
    "raw_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of people per type are very imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"posts\"] = df.posts.apply(lambda l: l.split(\"|||\"))\n",
    "df[\"posts_count\"] = df.posts.apply(len)\n",
    "\n",
    "url_regex = r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,4}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\"\n",
    "punctuation_regex = r\"[.!?\\\\-]\"\n",
    "emoji_regex = \"(\\:\\w+\\:|\\<[\\/\\\\]?3|[\\(\\)\\\\\\D|\\*\\$][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[3DOPp\\@\\$\\*\\\\\\)\\(\\/\\|])(?=\\s|[\\!\\.\\?]|$)\"\n",
    "# matches\n",
    "\"\"\"\n",
    ":( :) :P :p :O :3 :| :/ :\\ :$ :* :@\n",
    ":-( :-) :-P :-p :-O :-3 :-| :-/ :-\\ :-$ :-* :-@\n",
    ":^( :^) :^P :^p :^O :^3 :^| :^/ :^\\ :^$ :^* :^@\n",
    "): (: $: *:\n",
    ")-: (-: $-: *-:\n",
    ")^: (^: $^: *^:\n",
    "<3 </3 <\\3\n",
    ":smile: :hug: :pencil:\n",
    "\"\"\"\n",
    "\n",
    "df[\"posts_without_urls\"] = df.posts \\\n",
    "    .apply(lambda posts: [re.sub(url_regex, '', post) for post in posts])\n",
    "\n",
    "# Counting occurences of things across all posts of a person\n",
    "df[\"urls_count\"] = df.posts \\\n",
    "    .apply(lambda posts: sum([len(re.findall(url_regex, post)) for post in posts]))\n",
    "df[\"punctuations_count\"] = df.posts_without_urls \\\n",
    "    .apply(lambda posts: sum([len(re.findall(punctuation_regex, post)) for post in posts]))\n",
    "df[\"emojis_count\"] = df.posts_without_urls \\\n",
    "    .apply(lambda posts: sum([len(re.findall(emoji_regex, post)) for post in posts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_count_means_per_person(df):\n",
    "    \"\"\"\n",
    "    Adds a mean per person column for each count column\n",
    "    Requires the \"posts_count\" column\n",
    "    \"\"\"\n",
    "    count_columns = [c for c in df.columns if c.endswith(\"_count\") and c != \"posts_count\"]\n",
    "    for count_column in count_columns:\n",
    "        mean_col_name = \"mean_{term_singular}_count\".format(term_singular=count_column.split(\"_\")[0][:-1])\n",
    "        df[mean_col_name] = df[count_column] / df.posts_count\n",
    "    return df\n",
    "        \n",
    "df = add_count_means_per_person(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the 4 dichotomies\n",
    "df['is_E'] = df['type'].apply(lambda x: 1 if x[0] == 'E' else 0)\n",
    "df['is_S'] = df['type'].apply(lambda x: 1 if x[1] == 'S' else 0)\n",
    "df['is_T'] = df['type'].apply(lambda x: 1 if x[2] == 'T' else 0)\n",
    "df['is_J'] = df['type'].apply(lambda x: 1 if x[3] == 'J' else 0)\n",
    "dichotomies = [\"is_E\", \"is_S\", \"is_T\", \"is_J\"]\n",
    "all_dichotomies = [\"is_{letter}\".format(letter=letter) for letter in list(\"EISNTFJP\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0].posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_from_individuals(by: [str]):\n",
    "    \"\"\"\n",
    "    Groups individuals by the specified column(s)\n",
    "    \"\"\"\n",
    "    aggregation = {\n",
    "        **{\"posts_count\": [sum, len],\n",
    "           \"posts\": lambda post_lists: ' '.join([post for posts in post_lists for post in posts])},\n",
    "          # flattens the list of post_lists then joins the result to have 1 huge string\n",
    "        **{mean_c: np.mean for mean_c in [c for c in df.columns if c.startswith(\"mean_\")]}\n",
    "    }\n",
    "    df_per_type = df.groupby(by) \\\n",
    "        .agg(aggregation).rename(columns={\"posts\": \"content\"})\n",
    "    df_per_type[\"individuals\"] = df_per_type.posts_count.len\n",
    "    df_per_type[\"posts\"] = df_per_type.posts_count[\"sum\"]\n",
    "    df_per_type.columns = df_per_type.columns.droplevel(1)\n",
    "    df_per_type = df_per_type.drop(columns=[\"posts_count\"])\n",
    "    df_per_type = df_per_type[list(df_per_type.columns[-2:]) + list(df_per_type.columns[:-2])]\n",
    "    return df_per_type\n",
    "\n",
    "df_per_type = aggregate_from_individuals(by=[\"type\"])\n",
    "df_per_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding dichotomies agg\n",
    "df_per_dicho = pd.concat([aggregate_from_individuals(by=dicho) for dicho in dichotomies])\n",
    "df_per_dicho.index = all_dichotomies\n",
    "df_per_dicho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining to have one summary df\n",
    "agg_df = pd.concat([df_per_type, df_per_dicho])\n",
    "agg_df.drop(columns=[\"content\"]).to_csv(\"some_stats_per_group.csv\")\n",
    "agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_to_plot in [c for c in agg_df.columns if c.startswith(\"mean_\")]:\n",
    "    agg_df[col_to_plot].sort_values().plot.barh(title=col_to_plot.replace('_', ' ').title());\n",
    "    plt.savefig(\"{}.png\".format(col_to_plot), bbox=\"tight\");\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(url_regex, '', text)\n",
    "    text = re.sub(emoji_regex, '', text)\n",
    "    text = re.sub(punctuation_regex, '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W|_', ' ', text) # removes special chars\n",
    "    text = re.sub(r'[0-9]', '', text) # removes digits\n",
    "    text = re.sub(r'\\s+', ' ', text) # removes multiple spaces\n",
    "    text = re.sub(r'^\\s|\\s$', '', text) # removes space at the start or end of the string\n",
    "    \n",
    "    tokens = word_tokenize(text)  # tokenizes\n",
    "    \n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    tokens = [token for token in tokens if token not in stopwords] # removes stopwords\n",
    "    \n",
    "    wn = nltk.WordNetLemmatizer()\n",
    "    tokens = [wn.lemmatize(token) for token in tokens] # lematizes=root words\n",
    "    return tokens\n",
    "\n",
    "example_post = agg_df.iloc[0].content[:174]\n",
    "print(\"{} \\n     |\\n     v\\n{}\".format(\n",
    "    example_post,\n",
    "    clean_text(example_post)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df.content = agg_df.content.apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Words\n",
    "All words should not be kept as some tend to make a model worse rather than better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary_and_occurences(series):\n",
    "    series = series.copy()\n",
    "    all_text = []\n",
    "    for text in series:\n",
    "        for word in text:\n",
    "            all_text.append(word)\n",
    "            \n",
    "    vocab = set(all_text)\n",
    "    occurences_per_word = {word: all_text.count(word) for word in vocab}\n",
    "    return vocab, occurences_per_word\n",
    "\n",
    "vocabulary, occurencer_per_word = get_vocabulary_and_occurences(agg_df.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"word_occurrences.json\", \"w\") as outfile:  \n",
    "    json.dump(occurencer_per_word, outfile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurencer_per_word_series = pd.Series(list(occurencer_per_word.values()))\n",
    "occurencer_per_word_series.index = occurencer_per_word.keys()\n",
    "occurencer_per_word_series.plot.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_OCCURENCES = 2\n",
    "rare_words = [w for w in vocabulary if occurencer_per_word[w] < MIN_OCCURENCES]\n",
    "print(\"{} words ({}%) will not be kept because of their rarity\".format(\n",
    "    len(rare_words),\n",
    "    round(len(rare_words) / len(vocabulary) * 100)\n",
    "))\n",
    "\n",
    "def remove_rare_words(tokens):\n",
    "    return [t for t in tokens if t not in rare_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn machine learning models are meant to process numerical data, not text.\n",
    "\n",
    "We will therefore vectorize our text: we will convert them to a table indicating for each one the presence or absence of each word with a float value translating its relative importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_vectors_df(text_col): \n",
    "    tfidf_vect = TfidfVectorizer()\n",
    "    X = tfidf_vect.fit_transform(text_col)\n",
    "    df = pd.DataFrame(X.toarray())\n",
    "    df.columns = tfidf_vect.get_feature_names()\n",
    "    return df\n",
    "    \n",
    "vectors = get_vectors_df(agg_df.content.apply(lambda c: ' '.join(c)))\n",
    "vectors[\"group\"] = agg_df.index\n",
    "vectors = vectors[[\"group\"] + list(vectors.columns[1:])]\n",
    "vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing non-correlated words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = vectors.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_indexes = source_dummies.columns\n",
    "corr_df = corr_df[char_indexes].sort_values(0, ascending=False)\n",
    "corr_df = corr_df.drop(char_indexes)\n",
    "corr_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
